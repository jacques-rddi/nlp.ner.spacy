{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0243e0d6",
   "metadata": {},
   "source": [
    "# Refomat data from CoNLL 2003\n",
    "\n",
    "CoNLL 2003 is an anotated dataset for name entity recognition. It contains the following types of entities:\n",
    "- Person\n",
    "- Location\n",
    "- Organisation\n",
    "- Groupe of people\n",
    "\n",
    "We will use this dataset in order to test the performance of the NER model based on transformer provided by the open source framework [spaCy](https://spacy.io/).\n",
    "\n",
    "The format of the data was idea to use it in the model. Therefore we had o reformat it. This is the purpose of this notebook.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7109cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71ae96",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce4be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../data/conll2003.txt\", \"r\")\n",
    "whole_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833a1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU\tB-ORG\n",
      "rejects\tO\n",
      "German\tB-MISC\n",
      "call\tO\n",
      "to\tO\n",
      "boycott\tO\n",
      "British\tB-MISC\n",
      "lamb\tO\n",
      ".\tO\n",
      "\n",
      "Peter\tB-PER\n",
      "Blackburn\tI-PER\n",
      "\n",
      "BRUSSELS\tB-LOC\n",
      "1996-08-22\tO\n",
      "\n",
      "The\tO\n",
      "Euro\n"
     ]
    }
   ],
   "source": [
    "print(whole_text[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875b655",
   "metadata": {},
   "source": [
    "## Text processing\n",
    "### Sperating the sentences, the tokens and the word from the associated entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db2c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentence which are seperated by two concecutive new lines\n",
    "sentences = whole_text.split(\"\\n\\n\")\n",
    "# Seperate the token which are seperated by a single newlines\n",
    "# Seperate the word from the entity which are seperated by a tab\n",
    "tokenized_sentences = [[t.split(\"\\t\") for t in s.split(\"\\n\")] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05cb108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['EU', 'B-ORG'],\n",
       "  ['rejects', 'O'],\n",
       "  ['German', 'B-MISC'],\n",
       "  ['call', 'O'],\n",
       "  ['to', 'O'],\n",
       "  ['boycott', 'O'],\n",
       "  ['British', 'B-MISC'],\n",
       "  ['lamb', 'O'],\n",
       "  ['.', 'O']],\n",
       " [['Peter', 'B-PER'], ['Blackburn', 'I-PER']],\n",
       " [['BRUSSELS', 'B-LOC'], ['1996-08-22', 'O']]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d4424",
   "metadata": {},
   "source": [
    "### Compute the index of first and last letter of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9038aeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a060cc81c614f4c864c3156e7c5cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_sentence_and_list_of_ents = []\n",
    "# Loop over the sentences\n",
    "for sentence in tqdm(tokenized_sentences[:-2]):\n",
    "    # We rebuild the sentence bit by bit, so we start with an empty string\n",
    "    string_sentence = \"\"\n",
    "    # We keep in memory the current entoty type to see when it changes\n",
    "    cureent_ent = 'O'\n",
    "\n",
    "    # For each new sentence we initialise an empty list of entities\n",
    "    list_of_entities = []\n",
    "    # Each entity will be described by a dictionairy\n",
    "    current_entity_dict = {}\n",
    "    # We need to know if the current token is the first token to manage the\n",
    "    # space between the tokens in the sentence\n",
    "    first_token = True\n",
    "    for t_ent  in sentence:\n",
    "        # Get the next token and it's entity type\n",
    "        t, ent = t_ent[0], t_ent[1].split('-')[-1]\n",
    "        # Unless the current token is the first one or it starts with an appostrophy\n",
    "        # we add a space at the end of the sentence before adding the token\n",
    "        if (not first_token) and (not \"'\" == t[0]):\n",
    "            string_sentence += \" \"\n",
    "        # We add the token to the sentence\n",
    "        string_sentence += t\n",
    "        # If we just changed type of current entity\n",
    "        if cureent_ent != ent:\n",
    "            # if we end a meaningfull entity ( O means no entity) we complete \n",
    "            # its coresponding dictionnairy\n",
    "            if cureent_ent != 'O':\n",
    "                # We get the index of the last character of the entity in the sentence\n",
    "                current_entity_dict[\"end\"] = len(string_sentence) - len(t) - 1\n",
    "                # We add the dictionnairy to the list of entities\n",
    "                list_of_entities.append(copy.deepcopy(current_entity_dict))\n",
    "                # We create a new dictionnairy for the new entity\n",
    "                current_entity_dict = {}\n",
    "            # if we start a meaningfull entity, we start putting information in \n",
    "            # its dictionnairy\n",
    "            if ent != 'O':\n",
    "                # We add the type of the ew enetity\n",
    "                current_entity_dict['ent'] = ent\n",
    "                # We get the index of the first character of the entity in the sentence\n",
    "                current_entity_dict['start'] = len(string_sentence) - len(t)\n",
    "        # We update current entity\n",
    "        cureent_ent = ent\n",
    "        # We indicate that the current token will no longer be the first one\n",
    "        first_token = False\n",
    "    # At the end of the sentence we deal with the potential last entity\n",
    "    if cureent_ent != 'O':\n",
    "                current_entity_dict[\"end\"] = len(string_sentence)\n",
    "                list_of_entities.append(copy.deepcopy(current_entity_dict))\n",
    "                current_entity_dict = {}\n",
    "    # We add the list of entity corresponding to the current sentence in the dataset\n",
    "    all_sentence_and_list_of_ents.append({\"string\": string_sentence, \"ents\": list_of_entities})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33672451",
   "metadata": {},
   "source": [
    "### We have a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38514d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'string': 'EU rejects German call to boycott British lamb .',\n",
       "  'ents': [{'ent': 'ORG', 'start': 0, 'end': 2},\n",
       "   {'ent': 'MISC', 'start': 11, 'end': 17},\n",
       "   {'ent': 'MISC', 'start': 34, 'end': 41}]},\n",
       " {'string': 'Peter Blackburn',\n",
       "  'ents': [{'ent': 'PER', 'start': 0, 'end': 15}]},\n",
       " {'string': 'BRUSSELS 1996-08-22',\n",
       "  'ents': [{'ent': 'LOC', 'start': 0, 'end': 8}]},\n",
       " {'string': 'The European Commission said on Thursday it disagreed with German advice to consumers to shun British lamb until scientists determine whether mad cow disease can be transmitted to sheep .',\n",
       "  'ents': [{'ent': 'ORG', 'start': 4, 'end': 23},\n",
       "   {'ent': 'MISC', 'start': 59, 'end': 65},\n",
       "   {'ent': 'MISC', 'start': 94, 'end': 101}]},\n",
       " {'string': \"Germany's representative to the European Union's veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .\",\n",
       "  'ents': [{'ent': 'LOC', 'start': 0, 'end': 6},\n",
       "   {'ent': 'ORG', 'start': 32, 'end': 45},\n",
       "   {'ent': 'PER', 'start': 70, 'end': 86},\n",
       "   {'ent': 'LOC', 'start': 162, 'end': 169}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_and_list_of_ents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906097c",
   "metadata": {},
   "source": [
    "#### We verify if the index of the entities are correct for the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96eca1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_and_list_of_ents[0]['string'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bee27a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_and_list_of_ents[0]['string'][11:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e5a5c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'British'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentence_and_list_of_ents[0]['string'][34:41]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18124ca",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "It seems all good.\n",
    "\n",
    "## Save the reformated dataset into pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6526d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../data/ner_testing.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(all_sentence_and_list_of_ents,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63ddac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
